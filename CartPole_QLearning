import tensorflow as tf
import numpy as np
import random
import gym
import math
import matplotlib.pyplot as plt
from collections import deque

seed=12
tf.set_random_seed(seed)

class DQN():
    # Deep Q-Learning Network
    
    def __init__(self,env):  #initial the date
        
        self.global_step=tf.Variable(0, dtype=tf.int32, trainable=False, name='global_step')
        self.epsilon = 1
        self.replay_buffer = deque()
        self.create_placeholders()
        self.create_DQN()
        self.create_loss()
        self.create_optimiser()
        #self.train_Q_network()
        self.ep()
        self.sess = tf.InteractiveSession()
        self.sess.run(tf.global_variables_initializer())
        
    def create_placeholders(self):
        with tf.name_scope('data'):
            self.states=tf.placeholder(tf.float32, shape=(None,4), name="states")
            self.actions=tf.placeholder(tf.float32, shape=(None,2), name="actions")
            self.targetQ=tf.placeholder(tf.float32, shape=(None), name="targetQ")

    def _weight_variable(self, shape, name='', dtype=tf.float32):
        """weight_variable generates a weight variable of a given shape."""
        initial = tf.truncated_normal(shape, stddev=0.1)
        return tf.Variable(initial,dtype=dtype,name=name)
    
    def _bias_variable(self, shape, name='', dtype=tf.float32):
        """bias_variable generates a bias variable of a given shape."""
        initial = tf.constant(0.01, shape=shape)
        return tf.Variable(initial,dtype=dtype,name=name)
            
    def create_DQN(self):
        W_fc1 = self._weight_variable([4, 20],name='fc1',dtype=tf.float32)
        b_fc1 = self._bias_variable([20],name='fc1',dtype=tf.float32)
        a_fc1 = tf.nn.relu(tf.matmul(self.states, W_fc1) + b_fc1)

        W_fc2 = self._weight_variable([20, 2],name='fc2',dtype=tf.float32)
        b_fc2 = self._bias_variable([2],name='fc2',dtype=tf.float32)
        self.Q_predicted = tf.matmul(a_fc1, W_fc2) + b_fc2
        
    def create_loss(self):
        with tf.name_scope('loss'):
            Q_pred = tf.reduce_sum(tf.multiply(self.Q_predicted,self.actions),reduction_indices = 1)
            self.loss = tf.reduce_mean(tf.square(self.targetQ - Q_pred))

    def create_optimiser(self):
        with tf.name_scope('optimiser'):
            #self.optimizer = tf.train.GradientDescentOptimizer(**opt_kwargs).minimize(self.loss,global_step=self.global_step) 
            self.optimizer = tf.train.AdamOptimizer(0.001).minimize(self.loss,global_step=self.global_step)
    
    def perceive(self,state,action,reward,next_state,done):
        one_hot_action = np.zeros(2)
        one_hot_action[action] = 1
        self.replay_buffer.append((state,one_hot_action,reward,next_state,done))
        if len(self.replay_buffer) > 10000:
            self.replay_buffer.popleft()
        if len(self.replay_buffer) > 200:
            self.train_Q_network()
    
    def train_Q_network(self):
        minibatch = random.sample(self.replay_buffer,100)
        state_batch = [data[0] for data in minibatch]
        action_batch = [data[1] for data in minibatch]
        reward_batch = [data[2] for data in minibatch]
        next_state_batch = [data[3] for data in minibatch]
        
        targetq = []
        predqnext = self.Q_predicted.eval(feed_dict={self.states:next_state_batch})
        for i in range(0,100):
            done = minibatch[i][4]
            if done:
                targetq.append(reward_batch[i])
            else :
                targetq.append(reward_batch[i] + 0.9 * np.max(predqnext[i]))
        self.optimizer.run(feed_dict={
          self.targetQ:targetq,
          self.actions:action_batch,
          self.states:state_batch
          })
    
    def action(self,state):
        return np.argmax(self.Q_predicted.eval(feed_dict = {self.states:[state]})[0])
    
    def egreedy_action(self,state):
        if random.random() <= self.epsilon:
            return random.randint(0,1)
        else:
            return np.argmax(self.Q_predicted.eval(feed_dict = {self.states:[state]})[0])
        
        
    def ep(self):
        self.epsilon -= 1/10000
        return self.epsilon
   
   
env = gym.make ('CartPole-v0')
agent = DQN(env)
data1 = []
data2 = []
data3 = []
for episode in range(10000):
    state = env.reset()
    # Train
    for step in range(300):
        if step % 5 == 0:
            epsilon = agent.ep()
        action = agent.egreedy_action(state) # e-greedy action for train
        next_state,reward,done,_ = env.step(action)
        agent.perceive(state,action,reward,next_state,done)
        state = next_state
        if done:
            break
    # Test every 100 episodes
    if episode % 50 == 0:
        total_reward = 0
        for i in range(50):
            state = env.reset()
            for j in range(200):
                env.render()
                action = agent.action(state) # direct action for test
                state,reward,done,_ = env.step(action)
                total_reward += reward
                if done:
                    break
        ave_reward = total_reward/50
        print ('episode: ',episode,'epsilon: ',epsilon,'Evaluation Average Reward:',ave_reward)
        data1.append(episode)
        data2.append(epsilon)
        data3.append(ave_reward)
#        if ave_reward >= 200:
#           break
        if epsilon < 0:
            break
            
%matplotlib notebook
p1=plt.plot(data1,data3, "o",ms=15)
axes = plt.gca()
#axes.set_ylim([-7,7])

#handles, labels=axes.get_legend_handles_labels()
#plt.legend(handles,labels, loc='lower center')
plt.xlabel("$Traning Episode$")
plt.ylabel("$Evaluation Average Reward$")
plt.tight_layout()
plt.show()
